# Load your massive dataset
load("kenny_full_compensation_study_2025.RData")
# Check the basic structure
cat("Study completed on:", as.character(comprehensive_simulation_results$completion_time), "\n")
cat("Total simulations:", length(comprehensive_simulation_results$results), "\n")
cat("Runtime:", comprehensive_simulation_results$runtime_hours, "hours\n")
cat("Design matrix dimensions:", nrow(comprehensive_simulation_results$design_matrix), "x", ncol(comprehensive_simulation_results$design_matrix), "\n")
# Quick peek at the first few results
head(comprehensive_simulation_results$results[[1]])
# Load all the new modules
source("R/model.R")
source("R/simulate.R")
source("R/reliability.R")
source("R/fitting.R")
source("R/utils.R")           # New reliability-focused framework
source("R/visualization.R")   # New diagnostic visualization
source("R/compensation.R")    # New reliability diagnostic analysis
# Test the new reliability diagnostic approach (5 minutes)
diagnostic_test <- test_reliability_diagnostic(
model_type = "quadratic",
n_datasets = 100,         # 100 datasets from same true model
sample_size = 100
)
# Test the visualization
viz_test <- test_reliability_diagnostic_visualization(diagnostic_test)
1
# Run the full reliability study (8-24 hours)
full_study <- run_reliability_diagnostic_study(
study_size = "full",
use_parallel = TRUE
)
# Load the full study results
load("./reliability_diagnostic_study_full_2025-07-04.RData")
# Quick inspection
cat("Study completed with", length(reliability_study_results$results), "conditions\n")
cat("Runtime:", reliability_study_results$runtime_hours, "hours\n")
cat("Success rate:", (reliability_study_results$study_metadata$successful_conditions /
reliability_study_results$study_metadata$total_conditions) * 100, "%\n")
# Check structure
str(reliability_study_results, max.level = 2)
# Analyze failed conditions
failed_conditions <- sapply(reliability_study_results$results, function(x) isTRUE(x$error_occurred))
cat("Failed conditions:", sum(failed_conditions), "out of", length(failed_conditions), "\n")
# What types of conditions failed?
failed_indices <- which(failed_conditions)
if(length(failed_indices) > 0) {
failed_design <- reliability_study_results$design_matrix[failed_indices, ]
table(failed_design$model_comparison)  # Which model types failed?
table(failed_design$sample_size)       # Which sample sizes?
table(failed_design$error_variance)    # Which error levels?
}
# Overall diagnostic performance by model comparison
library(dplyr)
# Summary statistics by model type
summary_stats <- structured_data %>%
group_by(model_comparison) %>%
summarise(
n_conditions = n(),
mean_intercept_snr_degradation = mean(abs(intercept_snr_degradation), na.rm = TRUE),
mean_intercept_bias_difference = mean(abs(intercept_bias_difference), na.rm = TRUE),
strong_diagnostic_rate = mean(abs(intercept_snr_degradation) > 0.2, na.rm = TRUE),
.groups = 'drop'
)
# Use your existing function to structure the data
structured_data <- extract_reliability_study_data(reliability_study_results)
# Load the main execution script
source("final_execution.R")
# Test everything works
test_results <- quick_test_framework()
# Load the main execution script
source("execution.R")
# Load the main execution script
source("execution.R")
# Load the main execution script
source("execution.R")
# Load the main execution script
source("model.R")
